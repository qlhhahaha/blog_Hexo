---
title: 论文学习：A survey of distributed optimization
date: 2023-09-10
tags: [分布式优化, 多智能体控制]
categories: 科研只为把业毕


---



# A survey of distributed optimization, 2019, Annual Reviews in Control

### 一、introduction

1. **分布式优化**
	$$
	\min_{x\in\mathbb{R}^n}\sum_{i=1}^Nf_i(x),
	$$
	

### 二、离散时间分布式优化算法

1. **递减步长（diminishing step-sizes）**

	**DGD: Distributed Gradient Descent**
	$$
	x_i(k+1)=\sum_{j=1}^Nw_{ij}(k)x_j(k)-\alpha(k)s_i(k),
	$$
	其中$$w_{ij}(k)$$是边的权重，$$s_i(k)$$是局部函数$$f_i(x)$$的（次）梯度，$$\alpha(k)>0$$是递减的步长，其满足：
	$$
	\begin{aligned}&\sum_{k=0}^\infty\alpha\left(k\right)=\infty,\sum_{k=0}^\infty\alpha^2(k)<\infty,\\&\alpha\left(k\right)\leq\alpha\left(s\right)\mathrm{~for~all~}k>s\geq0.\end{aligned}
	$$
	

2. **固定步长（fixed step-sizes）**

​	**EXTRA: Exact first-order algorithm**

​	第一步：
$$
x_i(1)=\sum_{j=1}^Nw_{ij}x_j(0)-\alpha\nabla f_i(x_i(0)),
$$
​	其中$$\alpha>0$$是固定步长

​	第二步：
$$
\begin{aligned}x_i(k+2)&=x_i(k+1)+\sum_{j=1}^Nw_{ij}x_j(k+1)-\sum_{j=1}^N\tilde{w}_{ij}x_j(k)\\&-\alpha\left(\nabla f_i(x_i(k+1))-\nabla f_i(x_i(k))\right),\mathrm{~}k=0,\mathrm{~}1,\ldots,\end{aligned}
$$
​	相较于DGD算法，EXTRA用到了前两步的梯度信息，文献表明，**EXTRA可以看作是具有误差校正项的DGD**



​	**DIGing： distributed inexact gradient method and the gradient tracking**、
$$
\begin{aligned}
&x_i(k+1) =\sum_{j=1}^Nw_{ij}x_j(k)-\alpha y_i(k),  \\
&y_{i}(k+1) =\sum_{i=1}^Nw_{ij}y_j(k)+\nabla f_i(x_i(k+1))-\nabla f_i(x_i(k)), 
\end{aligned}
$$


​	**Distributed PI algorithm**
$$
\begin{aligned}
&x_i(k+1) =x_i(k)-v_i(k)-\alpha\nabla f_i(x_i(k))-\beta\sum_{j\in\mathcal{N}_i}a_{ij}(x_i(k)-x_j(k)), \\
&\nu_i(k+1) =v_i(k)+\alpha\beta\sum_{j\in\mathcal{N}_i}a_{ij}(x_i(k)-x_j(k)), 
\end{aligned}
$$
