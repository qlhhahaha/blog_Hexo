---
title: è®ºæ–‡å­¦ä¹ ï¼šA survey of distributed optimization
date: 2023-09-10
tags: [åˆ†å¸ƒå¼ä¼˜åŒ–, å¤šæ™ºèƒ½ä½“æ§åˆ¶]
categories: ç§‘ç ”åªä¸ºæŠŠä¸šæ¯•
---



ä¸€äº›å‚è€ƒï¼š

[Distributed Optimization Algorithms](https://forshining.github.io/posts/2022/07/blog-post-DistriOptim/)



# A survey of distributed optimization, 2019, Annual Reviews in Control

### ä¸€ã€introduction

1. **åˆ†å¸ƒå¼ä¼˜åŒ–**
	$$
	\min_{x\in\mathbb{R}^n}\sum_{i=1}^Nf_i(x),
	$$
	

### äºŒã€ç¦»æ•£æ—¶é—´åˆ†å¸ƒå¼ä¼˜åŒ–ç®—æ³•

1. **é€’å‡æ­¥é•¿ï¼ˆdiminishing step-sizesï¼‰**

	**DGD: Distributed Gradient Descent**
	$$
	x_i(k+1)=\sum_{j=1}^Nw_{ij}(k)x_j(k)-\alpha(k)s_i(k),
	$$
	å…¶ä¸­$w_{ij}(k)$æ˜¯è¾¹çš„æƒé‡ï¼Œ$s_i(k)$æ˜¯å±€éƒ¨å‡½æ•°$f_i(x)$çš„ï¼ˆæ¬¡ï¼‰æ¢¯åº¦ï¼Œ$\alpha(k)>0$æ˜¯é€’å‡çš„æ­¥é•¿ï¼Œå…¶æ»¡è¶³ï¼š
	$$
	\sum_{k=0}^\infty\alpha\left(k\right)=\infty,\sum_{k=0}^\infty\alpha^2(k)<\infty,
	$$

	$$
	\alpha\left(k\right)\leq\alpha\left(s\right)\mathrm{~for~all~}k>s\geq0.
	$$

	DGDç®—æ³•æœ¬è´¨ä¸Šæ˜¯å°†ä¼˜åŒ–è¿‡ç¨‹åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯consensus,å³åˆ©ç”¨Weight matrixå°†è¿é€šèŠ‚ç‚¹çš„ä¿¡æ¯åšä¸€ä¸ªæ²Ÿé€šï¼›å¦ä¸€éƒ¨åˆ†å°±æ˜¯ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™ï¼Œè¿™é‡Œçš„æ¢¯åº¦ä¸‹é™æ˜¯é’ˆå¯¹æ¯ä¸€ä¸ªlocalèŠ‚ç‚¹ã€‚è€ŒDGDæœ‰ä¸€ä¸ªæ˜¾è‘—çš„ç¼ºç‚¹ï¼šå¦‚æœDGDå½“ä¸­çš„æ­¥é•¿é€‰æ‹©å¸¸æ•°çš„è¯ï¼Œé‚£ä¹ˆä¼šå¾—åˆ°inexact convergenceï¼›è€Œå¦‚æœé€‰æ‹©é€æ¸è¶‹äºé›¶çš„æ­¥é•¿(diminishing step size)ï¼Œé‚£ä¹ˆè™½ç„¶å¯ä»¥å¾—åˆ°exact convergence, ä½†æ˜¯ä¼šé€ æˆè¾ƒæ…¢çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¿™åœ¨å®é™…åº”ç”¨å½“ä¸­æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„é—®é¢˜ã€‚

	```matlab
	% DGDä»¿çœŸ
	
	num_nodes = 5;  % èŠ‚ç‚¹æ•°é‡
	
	% æœ‰å‘å›¾
	adjacency_matrix_dir = [ 1  0 -1  0  0;
	                        -1  2  0  0 -1;
	                        -1 -1  2  0  0;
	                         0  0 -1  1  0;
	                         0  0  0 -1  1];  
	
	% æ— å‘å›¾
	adjacency_matrix_undir = [ 2 -1 -1  0  0;
	                          -1  2  0  0 -1;
	                          -1  0  2 -1  0;
	                           0  0 -1  2 -1;
	                           0 -1  0 -1  2];  
	
	
	% åˆå§‹åŒ–èŠ‚ç‚¹å‚æ•°å’Œæœ¬åœ°æ¢¯åº¦
	%node_params = rand(num_nodes, 1);  % åˆå§‹å‚æ•°
	node_params = [0.1 0.2 0.3 0.4 0.5]';
	local_gradients = zeros(num_nodes, 1);
	
	% è®¾ç½®å…¨å±€ç›®æ ‡å‡½æ•°ï¼ˆç¤ºä¾‹ä¸­ä¸ºç®€å•çš„å¹³æ–¹å’Œï¼‰
	global_objective = @(x) sum(x.^2);
	
	num_iterations = 1000;
	learning_rate = 0.01;
	
	% åˆ›å»ºç”¨äºå­˜å‚¨å¯è§†åŒ–æ•°æ®çš„æ•°ç»„
	param_history = zeros(num_iterations, num_nodes);
	
	
	for iteration = 1:num_iterations
	    for node = 1:num_nodes
	        local_gradients(node) = 2 * node_params(node);
	        
	        % ä¸é‚»å±…èŠ‚ç‚¹å…±äº«æ¢¯åº¦ä¿¡æ¯å¹¶æ›´æ–°å‚æ•°
	        for neighbor = 1:num_nodes
	            if adjacency_matrix_dir(node, neighbor) == -1
	                node_params(node) = node_params(node) - learning_rate * (node_params(node) - node_params(neighbor));
	            end
	        end
	        
	        % å­˜å‚¨å‚æ•°å†å²
	        param_history(iteration, :) = node_params;
	    end
	end
	
	% è¾“å‡ºæœ€ç»ˆçš„å…¨å±€æœ€ä¼˜è§£
	global_minimizer = node_params;
	global_minimum = global_objective(global_minimizer);
	fprintf('å…¨å±€æœ€ä¼˜è§£ï¼š');
	disp(global_minimizer);
	fprintf('å…¨å±€æœ€å°å€¼ï¼š%f\n', global_minimum);
	
	% å¯è§†åŒ–èŠ‚ç‚¹å‚æ•°éšæ—¶é—´çš„å˜åŒ–
	figure;
	hold on;
	for node = 1:num_nodes
	    plot(1:num_iterations, param_history(:, node), 'LineWidth', 2, 'DisplayName', sprintf('èŠ‚ç‚¹ %d', node));
	end
	xlabel('è¿­ä»£æ¬¡æ•°');
	ylabel('èŠ‚ç‚¹å‚æ•°');
	title('èŠ‚ç‚¹å‚æ•°éšæ—¶é—´çš„å˜åŒ–');
	legend('Location', 'Best');
	grid on;
	hold off;
	
	```

	ä»¿çœŸç»“æœï¼š

	![æ— å‘å›¾](https://s2.loli.net/2023/09/11/unY1li45ovgDGUR.png)

	æ— éœ€å¤ªå¤šè§£é‡Šï¼Œä¸ºæ— å‘å›¾å®Œå…¨è¿é€šæ—¶ï¼Œæœ€ç»ˆå„èŠ‚ç‚¹å€¼è¶‹äºå¹³å‡å€¼ï¼Œå³ç»å…¸çš„consensus

	![æœ‰å‘å›¾](https://s2.loli.net/2023/09/11/ST9zrA3w4no6muy.png)
	
	ä¸ºæœ‰å‘å›¾æ—¶ï¼Œå› ä¸ºä¸æ˜¯å„èŠ‚ç‚¹ä¹‹é—´éƒ½è¿˜èƒ½â€œå‡åŒ€åœ°â€é€šä¿¡ï¼Œæ‰€ä»¥ç»“æœä¹Ÿä¸å†æ˜¯å…¨å±€å¹³å‡
	
	

{% note warning%}

ğŸ’¡ PS. åªè¦èƒ½å¤Ÿæ”¶æ•›ï¼Œå„èŠ‚ç‚¹æœ€ç»ˆçš„æ”¶æ•›å€¼å°±ä¸ç›®æ ‡å‡½æ•°æ— å…³å“ˆï¼ˆè·Ÿå…¨å±€å’Œå±€éƒ¨ç›®æ ‡å‡½æ•°éƒ½æ— å…³ï¼Œå…¨å±€åªæ˜¯å±€éƒ¨ä¹‹å’Œè€Œå·²ï¼‰ï¼Œåªä¸é€šä¿¡æ‹“æ‰‘æœ‰å…³ã€‚å› ä¸ºä»è¿­ä»£è¿‡ç¨‹ä¸­å°±å¯ä»¥å‘ç°ï¼Œåªæœ‰æ¢¯åº¦å’Œç›®æ ‡å‡½æ•°æœ‰å…³ï¼Œä½†æ¢¯åº¦åªå½±å“è¿­ä»£é€Ÿç‡ï¼ˆæˆ–è€…è¯´é¡¶å¤šå¯¼è‡´éœ‡è¡ï¼Œæ— æ³•æ”¶æ•›åˆ°ç²¾ç¡®å€¼ï¼‰ï¼Œä¸å½±å“æ”¶æ•›ç»ˆå€¼

{% endnote %}



2. **å›ºå®šæ­¥é•¿ï¼ˆfixed step-sizesï¼‰**

	**EXTRA: Exact first-order algorithm**

	ç¬¬ä¸€æ­¥ï¼š
$$
x_i(1)=\sum_{j=1}^Nw_{ij}x_j(0)-\alpha\nabla f_i(x_i(0)),
$$

â€‹		å…¶ä¸­$$\alpha>0$$æ˜¯å›ºå®šæ­¥é•¿
â€‹	

â€‹		ç¬¬äºŒæ­¥ï¼š
$$
\begin{aligned}x_i(k+2)&=x_i(k+1)+\sum_{j=1}^Nw_{ij}x_j(k+1)-\sum_{j=1}^N\tilde{w}_{ij}x_j(k)-\alpha\left(\nabla f_i(x_i(k+1))-\nabla f_i(x_i(k))\right),\mathrm{~}k=0,\mathrm{~}1,\ldots,\end{aligned}
$$
â€‹		ç›¸è¾ƒäºDGDç®—æ³•ï¼ŒEXTRAç”¨åˆ°äº†å‰ä¸¤æ­¥çš„æ¢¯åº¦ä¿¡æ¯ï¼Œæ–‡çŒ®è¡¨æ˜ï¼Œ**EXTRAå¯ä»¥çœ‹ä½œæ˜¯å…·æœ‰è¯¯å·®æ ¡æ­£é¡¹çš„DGD**



â€‹		**DIGingï¼š distributed inexact gradient method and the gradient tracking**

$$
x_i(k+1) =\sum_{j=1}^Nw_{ij}x_j(k)-\alpha y_i(k),
$$

$$
y_{i}(k+1) =\sum_{i=1}^Nw_{ij}y_j(k)+\nabla f_i(x_i(k+1))-\nabla f_i(x_i(k))
$$



â€‹		**Distributed PI algorithm**
$$
x_i(k+1)=x_i(k)-v_i(k)-\alpha\nabla f_i(x_i(k))-\beta\sum_{j\in\mathcal{N}_i}a_{ij}(x_i(k)-x_j(k)),
$$

$$
\nu_i(k+1) =v_i(k)+\alpha\beta\sum_{j\in\mathcal{N}_i}a_{ij}(x_i(k)-x_j(k))
$$




3. **ä¸€é˜¶æ¢¯åº¦ç®—æ³•**

	**Distributed PI algorithm**
$$
\dot{x}_i(t) =\sum_{j=1}^Na_{ij}(x_j(t)-x_i(t))+\sum_{j=1}^Na_{ij}(\nu_j(t)-\nu_i(t))-\nabla f_i(x_i(t)),
$$

$$
\dot{\nu}_i(t) =\sum_{j=1}^Na_{ij}(x_i(t)-x_j(t))
$$

4. **äºŒé˜¶æ¢¯åº¦ç®—æ³•**

	**Zero-Gradient-Sum Algorithm**
$$
\dot{x}_i(t)=\gamma\left(\nabla^2f_i(x_i(t))\right)^{-1}\sum_{j\in\mathcal{N}_i}a_{ij}(x_j(t)-x_i(t))
$$


### å››ã€æ‹“å±•åœºæ™¯ä¸­çš„åˆ†å¸ƒå¼ä¼˜åŒ–ç®—æ³•

1. **æœ‰å‘å›¾**

  **Distributed Push-Sum Based Algorithmï¼ˆç¦»æ•£æ—¶é—´ï¼‰**

  å¤§éƒ¨åˆ†ç°æœ‰çš„æœ‰å‘å›¾ç¦»æ•£æ—¶é—´åˆ†å¸ƒå¼ç®—æ³•éƒ½æ˜¯åŸºäºæ¨å’Œ



  â€‹	

 2.  **æ—¶å»¶**

 3.  **éšæœºæ‹“æ‰‘**

 4.  **äº‹ä»¶è§¦å‘æœºåˆ¶**ï¼ˆç²¾è¯»ï¼ï¼‰

 5.  **æœ‰é™æ—¶é—´æ”¶æ•›**

	
